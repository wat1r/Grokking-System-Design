# 10. 设计 Twitter 搜索

> **难度等级：中等**

Twitter 是最大的社交网络服务之一，用户可以共享图像、新闻和基于文本的消息。这一章，我们将设计一个可以存储和搜索用户推文的服务。相似问题：推文搜索。

## 1. 什么是 Twitter 搜索？

Twitter 用户可以在任何时间更新他们的状态。每一条状态（称为推文）由纯文本组成，我们的目标是设计一个支持在所有的用户推文中搜索的系统。

## 2. 系统的要求和目标

- 我们假设 Twitter 有 150 亿总用户，8 亿日活用户。
- Twitter 平均每天有 4 亿条推文。
- 平均每条推文的大小是 300 个字节。
- 我们假设每天将有 5 亿次搜索。
- 搜索查询语句将包含多个单词，和 AND/OR 结合。

我们需要设计一个可以高效存储和查询推文的系统。

## 3. 容量估算和限制条件

**存储容量**：由于我们每天有 4 亿条新推文，平均每条推文的大小是 300 个字节，我们需要的总存储是：

400M * 300 => 120GB/day

每秒的总存储：

120GB / 24hours / 3600sec ~= 1.38MB/second

## 4. 系统 API

我们可以使用 SOAP 或 REST API 将我们的服务的函数公开。以下为搜索的 API 的定义：

```
search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token)
```

**参数**：

api_dev_key（string）：一个已注册的帐号的 API 开发者关键字。关键字将和其他字段一起根据用户分配的额度限制用户。
search_terms（string）：一个包含搜索项的字符串。
maximum_results_to_return（number）：返回的推文数量。
sort（number）：可选的排序模式：最近优先（0——默认）、最优匹配（1）、最多喜欢（2）。
page_token（string）：该标志表示结果集合中应该返回的页面。

**返回**：（JSON）
一个包含匹配查询语句的推文列表信息的 JSON 对象。每个结果想可以包含用户 ID 和名字、推文文本、推文 ID、创建时间、喜欢数量等。

## 5. 高阶设计

高阶角度，我们需要将所有的状态存入数据库，以及创建一个追踪每个单词在哪条推文中出现的索引。该索引可以帮助我们快速找到用户想搜索的推文。

![](/img/ch10_1.png)

## 6. 部件设计细节

**1. 存储**：我们需要每天存储 120GB 新数据。由于数据规模巨大，我们需要使用数据划分方案，高效地将数据分布到多台服务器上。如果我们为将来 5 年做计划，我们将需要的存储是：

120GB * 365days * 5years ~= 200TB

如果任何时候都不想超过总存储的 80%，我们大约需要 250TB 的总存储。我们假设需要存储所有推文的额外备份用于容错，则我们的总容量需求将是 500TB。如果我们假设一台现代服务器最多可以存储 4TB 的数据，我们将需要 125 台这样的服务器存储将来 5 年内所有需要的数据。

我们从一个明显简化的设计开始，将推文存在 MySQL 数据库中。我们可以假设我们将推文存在一张有两列的表中，两列分别是推文 ID 和推文文本。假设我们基于推文 ID 划分数据。如果推文 ID 是系统内唯一的，我们可以定义一个哈希函数，将推文 ID 映射到一台存储这条推文的存储服务器。

**我们如何创建系统中唯一的推文 ID**？如果我们每天接收 4 亿条新推文，我们在 5 年内可以预期多少个推文对象？

400M * 365 days * 5 years => 730 billion

这意味着我们需要使用 5 个字节的数字唯一地识别推文 ID。我们假设有一个服务可以在我们需要存储对象（这里讨论的推文 ID 和「设计 Twitter」中讨论的推文 ID 相似）的任何时候生成一个唯一的推文 ID。我们可以将推文 ID 传给哈希函数，找到存储服务器并将我们的推文对象存储在该服务器上。

**2. 索引**：我们的索引应该是什么样的？由于我们的推文查询由单词组成，我们建立的索引应该可以告诉我们哪个单词出现在哪个推文对象中。我们首先估算我们的索引有多大。如果我们想为所有的英语单词和一些著名的名词如人名、城市名等建立索引，以及如果我们假设有大约 30 万个英语单词和 20 万个名词，我们的索引中一共将有 50 万个单词。我们假设每个单词的平均长度是 5 个字符。如果我们将索引存储在内存中，我们需要 2.5MB 的内存存储所有的单词：

500K * 5 => 2.5 MB

我们假设只需要为过去 2 年的所有推文在内存中维护索引。由于我们在 5 年内将有 7300 亿条推文，在 2 年内将有 2920 亿条推文。已知每个推文 ID 的大小是 5 个字节，我们存储所有推文 ID 需要多少内存？

292B * 5 => 1460 GB

因此我们的索引将会像一个大型分布式哈希表，其中的关键字是单词，值是包含该单词的所有推文的推文 ID 列表。假设平均每条推文有 40 个单词，由于我们不需要索引介词和其他如 the、an、and 等小单词，我们假设每条推文中有大约 15 个单词需要被索引。这意味着每个推文 ID 将在我们的索引中存储 15 词。因此存储索引需要的总内存是：

(1460 * 15) + 2.5MB ~= 21 TB

假设一台高端服务器有 144GB 内存，我们需要 152 台这样的服务器存储我们的索引。

我们可以基于两个标准将我们的数据分片：

**基于单词分片**：当建立索引时，我们将遍历一条推文的全部单词并计算每个单词的哈希值以找到应该被索引的服务器。为了找到包含特定单词的全部推文，我们只需要在包含该单词的服务器上查询。

这个方法存在两个问题：

1. 如果一个单词很热门怎么办？这台服务器上将会有大量包含该单词的查询。高负载将影响影响服务的性能。

2. 随着时间的推移，一些单词可能比其他单词多存储很多推文 ID，因此，在推文持续增加的过程中维护单词的均匀分布是非常棘手的。

为了从上述状态中恢复，我们必须重新划分数据或者使用一致性哈希。

**基于推文对象分片**：存储时，我们将推文 ID 传递给哈希函数以找到服务器，并将推文中的所有单词在那台服务器上建立索引。当查询特定单词时，我们必须查询所有的服务器，每台服务器将返回一个推文 ID 的集合。一台中心化服务器将回合这些结果并将其返回给用户。

![](/img/ch10_2.png)

## 7. 容错

如果一台索引服务器宕机了怎么办？我们可以给每一台服务器安排一台二级备份服务器，当主服务器宕机时二级服务器可以在故障转移之后获得控制权。主服务器和二级服务器具有索引的相同备份。

如果主服务器和二级服务器同时宕机了怎么办？我们必须分配一台新服务器并在新服务器上重新建立相同的索引。我们如何做到？我们不知道这台服务器上存储了什么单词/推文。如果我们使用「基于推文对象分片」，暴力解决方法是遍历整个数据库并使用我们的哈希函数过滤推文 ID 以找到存储在这台服务器上的所有需要的推文。这个方法的效率低下，而且在重新建立服务器的时间内我们无法使用这台服务器处理任何查询，因此会丢失部分应该被用户看到的推文。

我们可以如何高效地获取推文和索引服务器之间的映射？我们必须建立一个反向索引，将所有的推文 ID 映射到对应的索引服务器。我们的索引建立服务器可以存储这些信息。我们需要建立一个哈希表，关键字是索引服务器数字，值是包含存储在这台索引服务器上的所有推文 ID 的的哈希集合。注意我们将所有的推文 ID 存入哈希集合，浙江允许我们快速在索引中添加/删除推文。此时，任何时候当一台索引服务器必须重新建立时，可以快速从索引建立服务器获得需要存储的所有推文，然后获取这些推文建立索引。这个方法的速度显然很快。我们也需要为索引建立服务器安排一台备份服务器用于容错。

## 8. 缓存

为了处理热门推文，我们可以在数据库前面引入缓存。我们可以使用 [Memcached](https://en.wikipedia.org/wiki/Memcached) 将所有的热门推文存入内存。在应用服务器访问后端数据库之前，可以快速检查缓存中是否有该推文。基于客户端的使用模式，我们可以调整需要多少台缓存服务器。对于缓存删除策略，最近最少使用（LRU）对我们的系统适用。

## 9. 负载均衡

我们可以在系统中的 2 个位置增加负载均衡层：1. 在客户端和应用服务器之间；2. 在应用服务器和后端服务器之间。初始时，可以使用一个简单的轮询调度方法，将进入地请求平均地分布到各台后端服务器。这样地负载均衡实现简单，不会引入新的开销。该方法的另一个好处是负载均衡会将宕机的服务器移出轮询，停止向其发送任何流量。轮询调度负载均衡的一个问题是没有考虑服务器的负载。如果一台服务器负载过重或者速度过慢，负载均衡并不会停止向这台服务器发送新请求。为了处理这个问题，可以使用一个更智能的负载均衡解决方案，该方案周期性地查询后端服务器获取负载信息，并基于该信息调整流量。

## 10. 搜索结果排名

如果我们要将搜索结果根据社交图距离、流行程度、相关程度等指标排名，应该怎么做？

假设我们要将推文根据流行程度排名，如一条推文获得多少个喜欢、多少条评论等。这种情况下，我们的排名系统可以计算一个「流行程度数」（基于喜欢的数量等）并将其存储在索引中。每个分区可以在将结果返回给汇合服务器之前基于这个流行程度数将结果排序。汇合服务器组合所有的结果，基于流行程度数将结果排序，然后将最靠前的结果发送给用户。









我们需要存储的数据包括用户、推文、喜欢的推文和被关注者。

Tweet 表

<table>
	<tr>
		<th colspan="2">Tweet</th>
	</tr>
	<tr>
		<td>PK</td>
		<td>TweetID: int</td>
	</tr>
	<tr>
		<td></td>
		<td>
			<div>UserID: int</div>
			<div>Content: varchar(140)</div>
			<div>TweetLatitude: int</div>
			<div>TweetLongitude: int</div>
			<div>UserLatitude: int</div>
			<div>UserLongitude: int</div>
			<div>CreationDate: datetime</div>
			<div>NumFavorites: int</div>
		</td>
	</tr>
</table>

User 表

<table>
	<tr>
		<th colspan="2">User</th>
	</tr>
	<tr>
		<td>PK</td>
		<td>UserID: int</td>
	</tr>
	<tr>
		<td></td>
		<td>
			<div>Name: varchar(20)</div>
			<div>Email: varchar(32)</div>
			<div>DateOfBirth: varchar(32)</div>
			<div>CreationDate: datetime</div>
			<div>LastLogin: datatime</div>
		</td>
	</tr>
</table>

UserFollow 表

<table>
	<tr>
		<th colspan="2">UserFollow</th>
	</tr>
	<tr>
		<td>PK</td>
		<td>
			<div>UserID1: int</div>
			<div>UserID2: int</div>
		</td>
	</tr>
</table>

Favorite 表

<table>
	<tr>
		<th colspan="2">Favorite</th>
	</tr>
	<tr>
		<td>PK</td>
		<td>
			<div>TweetID: int</div>
			<div>UserID: int</div>
		</td>
	</tr>
	<tr>
		<td></td>
		<td>
			<div>CreationDate: datetime</div>
		</td>
	</tr>
</table>

在选择 SQL 和 NoSQL 数据库存储上述模型方面，请参考「设计 Instagram」的「数据库模型」部分。

## 7. 数据分片

由于我们每天的新推文数量巨大，读负载也极高，我们需要将数据分布到多台机器上，使得我们的读写操作可以高效。我们有很多选项可以将数据分片，以下分别列举：

**基于用户 ID 分片**：我们可以尝试将一个用户的所有数据存储在一台服务器上。存储时，我们可以将用户 ID 传给我们的哈希函数，哈希函数将用户映射到一台数据库服务器，数据库服务器上存储所有用户发布的推文、喜欢的推文、关注者等信息。当查询一个用户发布的推文/关注者/喜欢的推文时，我们可以使用寻找用户数据的哈希函数从数据库服务器中读取相应的信息。这个方法存在一些问题：

1. 如果用户成为热门用户会如何？这样服务器上将会有大量关于该用户的查询。高负载将影响我们服务的性能。

2. 经过一段时间之后，一些用户和其他用户相比，可能发布大量推文或者关注大量用户。维护增长用户数据的均匀分配是非常困难的。

为了从这些情形中恢复，我们或者需要重新将数据分片，或者需要使用一致性哈希。

**基于推文 ID 分片**：我们的哈希函数将每个推文 ID 映射到随机的一台存储该推文的服务器。为了搜索推文，我们必须查询所有的服务器，每个服务器将返回一个推文集合。一个中心化服务器将聚集这些结果并返回给用户。我们用时间线的生成举例，以下是我们的系统生成用户时间线需要执行的操作：

1. 我们的应用服务器将找到用户关注的所有用户。

2. 应用服务器将查询发送到所有的数据库服务器，找到这些用户发布的推文。

3. 每个数据库服务器将找到每个用户的推文，将推文按照发布时间由近及远排序并返回最靠前的推文。

4. 应用服务器将所有的结果合并然后再次排序，将最靠前的结果返回给用户。

这个方法解决了热门用户的问题，但是不同于基于用户 ID 分片，我们必须查询所有的数据库分片才能找到一个用户的推文，这会导致延迟时间更高。

**基于推文创建时间分片**：基于创建时间存储推文的好处是可以快速获取所有的最靠前推文，而且我们只需要查询很小一部分服务器。问题是流量负载不会被分布到多台服务器，例如当写操作时，所有的新推文都进入一台服务器，其余服务器都是闲置的。类似地，当读操作时，存储最近数据的服务器和存储老数据的服务器相比，将有非常高的负载。

**我们是否可以结合基于推文 ID 和推文创建时间分片**？如果我们不单独存储推文创建时间，而是用推文 ID 反映创建时间，则可以结合两者的优势。使用这种方法可以很快找到最近发布的推文。为了实现这一点，我们必须在系统中将每个推文 ID 设为全局唯一，且每个推文 ID 也必须包含时间戳。

我们可以使用新纪元时间实现这一点。推文 ID 由两部分组成，第一部分是新纪元时间的秒数，第二部分是自动增加序列。因此，生成新的推文 ID 时，可以获取当前的新纪元时间然后将一个自动增加的数字添加在后面。我们可以从推文 ID 计算得到分片数字，并将推文存储在相应的分片中。

推文 ID 的大小时多少？假设新纪元时间从今天开始，存储将来 50 年的描述需要多少比特？

86400 sec/day * 365 (days a year) * 50 (years) => 1.6B

![](/img/ch6_2.png)

我们需要 31 个比特存储这个数字。由于平均每秒有 1150 条新推文，我们可以分配 17 个比特存储自动增加序列。推文 ID 的长度是 48 比特。因此每秒我们可以存储 2^17 => 13 万条新推文。我们可以在每秒将自动增加序列归零。考虑到容错和更好的性能，我们可以有两台数据库服务器生成自动增加关键字，一台服务器生成偶数关键字，另一台服务器生成计数关键字。

假设我们当前的新纪元时间秒数是 1483228800，推文 ID 如下所示：

1483228800 000001
1483228800 000002
1483228800 000003
1483228800 000004
……

如果我们将推文 ID 设为 64 比特（8 字节）长，我们可以存储将来 100 年的推文，也可以使用毫秒级的粒度存储。

上述方法中，生成时间线时我们仍然必须查询所有的服务器，但是读和写操作的速度会显著提升。

1. 由于我们没有任何二级索引（查询时间），因此可以减少写操作的延迟。

2. 读操作时，我们不需要基于创建时间过滤，因为主键包含了新纪元时间。

## 8. 缓存

我们可以为数据库服务器引入缓存，用于缓存热门推文和用户。我们可以使用现成的解决方案，例如 Memcache，存储整个推文对象。在访问数据库之前，应用服务器可以快速检查缓存中是否又目标推文。基于客户端的使用模式，我们可以决定需要多少缓存服务器。

**哪种缓存替换机制最适合我们的需求**？当缓存已满，我们需要将一条推文替换成更新/更热门的推文，我们应该如何选择？最近最少使用（LRU）可以是我们系统的一个合适的机制。该机制下，我们首先丢弃最近最少访问的推文。

**我们如何得到更智能的缓存**？如果我们遵循 80-20 法则，20% 的推文产生 80% 的读流量，即特定推文非常热门，大多数用户都会阅读。这点说明我们应该尝试缓存每个分片中的每天被阅读的推文的 20%。

**缓存最近数据如何**？这种方法有助于我们的服务。假设 80% 的用户只查看过去 3 天的用户，我们可以尝试缓存过去 3 天的全部推文。假设我们用缓存服务器缓存过去 3 天所有用户发布的推文。根据上述估算，我们每天又 1 亿条新推文或者 30GB 新数据（不包含图像和视频）。如果我们想存储过去 3 天的所有推文，我们需要的内存小于 100GB。这些数据可以存入一台服务器，但是我们应该将其复制到多台服务器上，以降低读流量，减少缓存服务器的负载。所以在任何时候当我们生成用户的时间线时，我们可以询问缓存服务器是否有该用户的所有最近推文。如果有，我们只要返回缓存中的全部数据即可。如果缓存中没有足够的推文，我们必须查询后端服务器获得数据。使用类似的设计，我们可以尝试缓存过去 3 天的图像和视频。

我们的缓存如同哈希表，关键字是用户 ID，值是双向链表，双向链表包含该用户过去 3 天的所有推文。由于我们想首先获得最近的数据，我们总是可以将新推文插入链表的头部，意味着所有的老推文都在链表的尾部附近。因此，我们可以从链表的尾部删除推文，为新推文留出空间。

![](/img/ch6_3.png)

## 9. 时间线生成

关于时间线生成的具体讨论，参考「设计 Facebook 的新闻推送」。

## 10. 备份和容错

由于我们的系统是重读的，我们可以为每个数据库分片设定多个二级数据库服务器。二级服务器只用于读流量。所有的写操作首先进入主服务器然后被复制到二级服务器。该模型也支持容错，任何时候当主服务器宕机了，可以使用二级服务器实现失效转移。

## 11. 负载均衡

我们可以在系统中的 3 个位置增加负载均衡层：1. 在客户端和应用服务器之间；2. 在应用服务器和数据库备份服务器之间；3. 在聚集服务器和缓存服务器之间。初始时，可以使用一个简单的轮询调度（Round Robin）方法，将进入的请求平均地分布到各台服务器。这样的负载均衡实现简单，不会引入新的开销。该方法的另一个好处是如果一台服务器宕机了，负载均衡可以将其移出轮询，停止向其发送任何流量。轮询调度负载均衡的一个问题是没有考虑服务器的负载。如果一台服务器负载过重或者速度过慢，负载均衡器并不会停止向这台服务器发送新请求。为了处理这个问题，可以使用一个更智能的负载均衡解决方案，该方案周期性地查询后端服务器获取负载信息，并基于该信息调整流量。

## 12. 监控

对于我们的系统的监控能力是至关重要的。我们应该持续手机数据以实时了解系统的运行情况。我们可以收集以下数值，了解我们服务的性能：

1. 每天/每秒的新推文数量，每天的峰值是多少？

2. 时间线发布状态，我们的系统每天/每秒发布多少条推文。

3. 用户可见的刷新时间线的平均延迟。

通过监视这些数值，我们可以发现是否需要更多的备份、负载均衡或缓存。

## 13. 扩展性要求

**我们如何处理信息流**？获取一个用户关注的用户的所有最新推文，然后按照时间合并/排序。使用页码获取/显示推文。只获取一个用户关注的所有用户的最靠前的 N 条推文。这里的 N 将取决于客户端的视口（Viewport），因为在手机上显示的推文数少于在网页上显示的推文数。我们也可以缓存后一部分靠前的推文以提升速度。

另一种做法是，我们可以实现生成信息流以提升效率。细节方面请参考「设计 Instagram」的「排名和时间线生成」部分。

**回复**：由于数据库中已有每条推文对象，我们可以在回复对象中存储原始推文的 ID 且不存储任何内容。

**热门话题**：我们可以缓存过去 N 秒内出现频率最多的话题标签或者查询语句，并持续每隔 M 秒更新一次。我们可以基于推文、查询语句、回复或喜欢的频率对热门话题排序。我们可以对展示给更多用户的话题赋予更多的权重。

**关注谁**？**如何推荐**？这个特征将改善用户关系。我们可以推荐某个用户关注的用户的朋友。我们可以深入 2 到 3 层寻找著名的用户作为推荐。我们可以给拥有更多粉丝的用户更多的倾向性。

由于任何时候只能有少数推荐，使用机器学习（ML）重新排列和调整优先级。机器学习标志可以包含最近粉丝数量增加的用户、关注当前用户的其他用户的共同关注、共同的地点或兴趣等。

**瞬间**：从不同网站获取过去 1 到 2 小时的热点新闻，找到相关推文，将这些推文优先级提升，使用机器学习——有监督学习或聚类的方式将这些推文分类（新闻、自主、金融、娱乐等）。然后我们可以将这些文章以热门话题的形式展示在瞬间中。

**搜索**：搜索包含索引、排序和获取推文。下一个问题「设计 Twitter 搜索」中讨论一个类似的方案。
